<!doctype html><html lang=es-mx><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><title>Cómo funcionan los transformers</title>
<link rel=stylesheet media=screen href=https://joshua.haase.mx/css/theme.min.2ee1317322f9eb9b2ef0a618d19b20e38c11f5f9c310751400a45db225dd2626.css integrity="sha256-LuExcyL565su8KYY0Zsg44wR9fnDEHUUAKRdsiXdJiY="></head><body><main><article><h1>Cómo funcionan los transformers</h1><nav id=TableOfContents><ul><li><a href=#características-de-los-transformers>Características de los transformers</a></li><li><a href=#qué-es-auto-atención>¿Qué es auto atención?</a></li><li><a href=#activación>Activación</a></li></ul></nav><div style=position:relative;padding-bottom:56.25%;overflow:hidden><iframe style=position:absolute;width:100%;height:100% src=https://www.youtube.com/embed/S27pHKBEp30 allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy=strict-origin-when-cross-origin allowfullscreen referrerpolicy=strict-origin-when-cross-origin frameborder=0></iframe></div></br><h2 id=características-de-los-transformers><a href=#caracter%c3%adsticas-de-los-transformers alt>Características de los transformers</a> <a href=# alt="Regresar al inicio">↑</a></h2><ul><li><p>La primer aproximación a NLP fue usando vectores de n-palabras de un lenguaje.
No funcionó por el orden y entonces los n-gramas pero requieren muchísimo espacio.</p></li><li><p>Se proponen los Recurrent Neural Network porque ¿pueden entrenarse con una entrada lineal? y usan mucho menos memoria.</p></li><li><p>Los RNN tienen el problema de explosión y dilución de factores porque funcionan como exponenciales.
LSTM resuelve ese problema.</p></li><li><p>El entrenamiento en LSTM no se puede transferir fácilmente.
<strong>Los transformers pueden transferir conocimiento</strong> a diferencia de los modelos anteriores.</p></li><li><p>Hay al menos dos artículos de transformers con nombre de Muppet <a href=https://github.com/yuanxiaosc/ELMo target=_blank rel=noopener>ELMo</a> y <a href=~/web/arxiv.org/pdf/1810.04805 title="Bidirectional Encoding Representation from Transformers">BERT</a></p></li></ul><p>¿Qué forma tienen los RNN?</p><p>¿Qué otras formas de datos pueden usarse?</p><p><a href=https://arxiv.org/pdf/1706.03762.pdf target=_blank rel=noopener>Attention is all you need</a></p><p><a href=~/web/arxiv.org/pdf/1810.04805>BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</a>
Bidirectional Encoder Representations from Transformers</p><p><a href=https://github.com/yuanxiaosc/ELMo target=_blank rel=noopener>Embeddings from Language Models (ELMo)</a>
<a href=https://arxiv.org/pdf/1802.05365v2.pdf target=_blank rel=noopener>Deep contextualized word representation</a></p><h2 id=qué-es-auto-atención><a href=#qu%c3%a9-es-auto-atenci%c3%b3n alt>¿Qué es auto atención?</a> <a href=# alt="Regresar al inicio">↑</a></h2><p>Ls atenciones son mapas entre claves y valores donde las entradas, salidas, búsquedas, claves y valores son vectores.</p><p><a href=https://arxiv.org/abs/1706.03762 target=_blank rel=noopener>Este paper debería explicarlo</a>.</p><h2 id=activación><a href=#activaci%c3%b3n alt>Activación</a> <a href=# alt="Regresar al inicio">↑</a></h2><p>Para la activación se utiliza una función llamada ReLU, que empieza en 0 y luego aumenta de manera lineal.
Permite a cada grupo expresar una idea única.</p><p><a href=https://arxiv.org/pdf/1606.08415.pdf target=_blank rel=noopener>ReLU</a> es robusta a los valores de activación, en los sigmoideos se tienen que elegir para estar en la parte media que es relevante.</p><ul><li><p>Activación no importa (si no usas sigmoideos y tanh).</p></li><li><p>Los optimizadores importan.</p></li></ul><p><a href=https://lifearchitect.ai/megatron/ target=_blank rel=noopener>Megatron</a></p><p><a href=https://arxiv.org/pdf/2005.14165.pdf target=_blank rel=noopener>GPT-3</a></p><p><a href=https://keras.io/api/optimizers/ target=_blank rel=noopener>Configurar optimizadores en keras</a></p></article></main></body></html>