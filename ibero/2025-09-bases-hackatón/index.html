<!doctype html><html lang=es-mx><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><title>Bases Hackatón</title><link rel=stylesheet media=screen href=https://joshua.haase.mx/css/theme.min.2ee1317322f9eb9b2ef0a618d19b20e38c11f5f9c310751400a45db225dd2626.css integrity="sha256-LuExcyL565su8KYY0Zsg44wR9fnDEHUUAKRdsiXdJiY="><script src=https://hypothes.is/embed.js></script></head><body><main><article><h1>Bases Hackatón</h1><nav id=TableOfContents></nav><p>Por favor ayúdame a redactar el texto de estas bases como una prosa con el estilo del ejemplo</p><h1 id=bases-oficiales--hackatón-detectives-de-datos><a href=#bases-oficiales--hackat%c3%b3n-detectives-de-datos alt>Bases oficiales — Hackatón &ldquo;Detectives de Datos&rdquo;</a> <a href=# alt="Regresar al inicio">↑</a></h1><ol><li><p>Título del evento</p><ul><li>Detectives de Datos</li><li>Opciones alternativas (para elección de comunicación):<ul><li>Detectives de Datos: Clasifica y Protege</li><li>Ibero Data Detectives</li><li>PhishGuard: Hackatón de Clasificación de Phishing</li></ul></li></ul></li><li><p>Organizadores</p><ul><li>Universidad Iberoamericana</li><li>Departamento: Ingeniería para la Innovación</li><li>Coordinación: Coordinación de Ciencia de Datos</li><li>Contactos organizadores: ver sección &ldquo;Contacto para aclaraciones&rdquo;</li></ul></li><li><p>Objetivo / descripción general</p><ul><li>Propósito: promover la conciencia sobre la importancia de la ciencia de datos y su impacto en la seguridad digital.</li><li>Competencia: análisis y clasificación de correos (dos clases: normal vs. phishing). Los equipos realizarán un análisis guiado y entregarán un reporte técnico y/o un modelo capaz de clasificar correos.</li><li>Público objetivo: estudiantes de nivel preparatoria (preparatorias de la Ciudad de México).</li></ul></li><li><p>Fechas y horarios</p><ul><li>Fecha única del hackatón (evento en sitio y/o en línea): 12 de noviembre, de 08:00 a 14:00 (CDMX).</li><li>Registro previo: hasta el 8 de noviembre, 23:59.</li><li>Publicación de resultados provisionales: 14 de noviembre, 18:00.</li><li>Período de apelaciones: 14–16 de noviembre (48 horas).</li><li>Resultados finales y entrega de premios: fecha y lugar anunciados tras cierre de apelaciones.</li></ul></li><li><p>Formato de la competencia</p><ul><li>Evento de una sola ronda (día único) para análisis y presentación.</li><li>Duración del evento en sitio: 6 horas (08:00–14:00).</li><li>Entregables obligatorios: notebook (Colab/Jupyter), informe técnico en PDF (máx. 5 páginas) y presentación (máx. 7 diapositivas).</li><li>Opcional: enlace a demo en Colab/Notebook ejecutable.</li></ul></li><li><p>Plataforma técnica</p><ul><li>Entorno recomendado: Google Colab y/o acceso a vega.ibero.mx (JupyterLab).</li><li>Se permiten herramientas externas: Tableau, RStudio, Python, R, Weka, etc.</li><li>Requerimiento: el notebook entregado debe ser reproducible (instrucciones de ejecución incluidas).</li></ul></li><li><p>Registro y acceso</p><ul><li>Registro obligatorio en: <a href=https://preuniversitarios.ibero.mx target=_blank rel=noopener>https://preuniversitarios.ibero.mx</a></li><li>Información requerida al registrarse: nombre del equipo, nombres completos y correos institucionales de integrantes, escuela de procedencia, asignar un responsable de contacto.</li><li>Plazo de registro: hasta el 8 de noviembre, 23:59.</li></ul></li><li><p>Composición y reglas de equipos</p><ul><li>Tamaño de equipo: mínimo 2, máximo 4 personas.</li><li>Elegibilidad: todos los integrantes deben ser estudiantes activos de preparatoria en la Ciudad de México y poder acreditar con credencial escolar vigente.</li><li>Sustituciones: se permiten hasta 1 sustitución por equipo antes del inicio del evento, siempre que se notifique y autorice por la organización.</li><li>Límite por institución: máximo 2 equipos elegibles para premiación/avance por la misma institución. (Ver sección “Aplicación del límite por institución”).</li></ul></li><li><p>Reglas de conducta y anti‑fraude</p><ul><li>Prohibido: registrarse en múltiples equipos, compartir una misma solución entre equipos, recibir ayuda externa pagada, suplantación de identidad, presentar trabajo ajeno sin atribución, uso no autorizado de IA (ver política de IA).</li><li>Consecuencias: penalización en puntaje, descalificación y prohibición de participar en futuras ediciones según gravedad.</li><li>Código de conducta: trato respetuoso en todo momento; lenguaje discriminatorio o comportamiento abusivo será causa de sanción.</li></ul></li><li><p>Política de Uso de Inteligencias Artificiales Generativas (resumen)</p></li></ol><ul><li>Uso permitido (soporte): redacción, sugerencias de visualización, apoyo conceptual.</li><li>Uso prohibido: generar directamente modelos, análisis clave, o respuestas sin revisión y entendimiento del equipo.</li><li>Requisito: declarar en el informe cualquier uso de IA (herramienta, propósito, validación).</li><li>Sanciones: penalización o descalificación por uso indebido.</li><li>Declaración estándar y ejemplo: ver plantilla de “Declaración de uso de IA” en anexos.</li></ul><ol start=11><li>Requerimientos técnicos y recomendaciones para participantes</li></ol><ul><li>Equipo: laptop personal con acceso a Internet.</li><li>Conectividad: se recomienda conexión estable; la organización ofrecerá red local y estaciones de trabajo cuando sea posible.</li><li>Software: posibilidad de instalar software; se recomienda contar con cuenta Google para Colab.</li><li>Salud: recomendamos pausas, hidratación y alimentación; la organización proveerá refrigerio según logística.</li></ul><ol start=12><li>Sistema de puntuación y clasificación (rubrica)</li></ol><ul><li>Puntaje total máximo: 500 puntos (escala explicativa).<ul><li>Rendimiento del modelo (métrica principal: F1-score en clase &ldquo;phishing&rdquo;, validación en hold-out o CV): 250 puntos (50%).</li><li>Calidad del análisis y EDA (insights, feature engineering, manejo de datos): 100 puntos (20%).</li><li>Reproducibilidad y código (notebook ejecutable, limpieza de código, documentación): 75 puntos (15%).</li><li>Reporte y presentación (claridad, justificación de decisiones, impacto): 50 puntos (10%).</li><li>Transparencia en uso de IA y ética (declaración y cumplimiento): 25 puntos (5%).</li></ul></li><li>Métricas específicas: F1-score de la clase phishing medido en dataset de evaluación retenido por la organización; se podrán usar métricas secundarias (AUC, precisión, recall) para desempate según el jurado.</li><li>Penalizaciones: plagio o uso indebido de IA o externalización pueden implicar deducción parcial o descalificación.</li></ul><ol start=13><li>Criterios y proceso de evaluación</li></ol><ul><li>Evaluación automática del rendimiento del modelo contra conjunto de prueba no divulgado.</li><li>Revisión manual por jurado de entregables (EDA, reproducibilidad, presentación).</li><li>Composición del jurado: especialistas en ciencia de datos e ingeniería de seguridad.</li><li>Los puntajes se normalizan y suman a la puntuación final.</li></ul><ol start=14><li>Criterios de desempate</li></ol><ul><li><ol><li>Mayor recall en clase &ldquo;phishing&rdquo; en el conjunto de evaluación.</li></ol></li><li><ol start=2><li>Modelo con menor complejidad (menos parámetros / menor tamaño del modelo).</li></ol></li><li><ol start=3><li>Mejor puntaje en sección de reproducibilidad y código.</li></ol></li><li><ol start=4><li>Fecha y hora de entrega (más temprano gana).</li></ol></li><li>Si persiste empate, decisión a criterio del jurado con justificación pública.</li></ul><ol start=15><li>Reglas específicas para finales / entrega de presentaciones</li></ol><ul><li>Formato similar a la clasificación.</li><li>Duración presentación final: 7 minutos + 3 minutos de preguntas.</li><li>Límite de integrantes en escenario: máximo 4.</li><li>Prohibido el uso de material no declarado o recibir asistencia en vivo de personal no registrado.</li></ul><ol start=16><li>Aplicación del límite por institución (resolución de inconsistencia)</li></ol><ul><li>Inscripción: abierta a múltiples equipos por escuela.</li><li>Clasificación: si en la lista de equipos finalistas (p. ej. top 15) hay más de 2 equipos de la misma escuela, sólo los dos mejor ubicados por esa escuela conservarán plaza para la final/premiación; las plazas vacantes se asignarán al siguiente equipo mejor clasificado de otra institución.</li><li>Esta regla será aplicada automáticamente por la organización antes de publicar la lista definitiva de finalistas.</li></ul><ol start=17><li>Proceso de apelaciones y reclamaciones</li></ol><ul><li>Plazo: 48 horas desde la publicación de resultados provisionales.</li><li>Procedimiento: enviar correo a <a href=mailto:appeals.ibero@ibero.mx>appeals.ibero@ibero.mx</a> con asunto “Apelación Detectives de Datos – [Equipo]”; incluir motivo detallado y evidencia.</li><li>Resolución: comité de apelaciones (no parte del jurado) emite resolución en un plazo máximo de 72 horas tras cierre de apelaciones. La decisión será final.</li></ul><ol start=18><li>Detección y penalización de automatización y explotación de sistemas</li></ol><ul><li>La organización monitoreará logs (accesos a notebooks proporcionados, versiones de código, HASH de archivos) y podrá requerir la ejecución en vivo de fragmentos de código.</li><li>Automatización no autorizada para generar resultados (bots, scripts de scraping que violen licencias) será motivo de sanción o descalificación.</li></ul><ol start=19><li>Reglas sobre uso de recursos externos y repositorios</li></ol><ul><li>Permitido: uso de documentación pública, Stack Overflow, repositorios de código público (GitHub) para consulta.</li><li>Prohibido: contratar o recibir ayuda técnica de terceros pagados; usar soluciones completas de terceros sin atribución; presentar trabajo descargado como propio.</li><li>Requisito: cualquier fragmento de código no original debe ser citado en el notebook y el informe.</li></ul><ol start=20><li>Herramientas permitidas / recomendadas y restricciones generales</li></ol><ul><li>Permitidas: Python (scikit-learn, TensorFlow, PyTorch), R, Colab, Jupyter, RStudio, Tableau, etc.</li><li>Recomendación: priorizar reproducibilidad y claridad.</li><li>Prohibidas: herramientas que realicen análisis/modelado completamente automatizado sin comprensión (según evaluación del jurado), asistencia humana externa no declarada.</li></ul><ol start=21><li>Entregables y procedimiento de envío</li></ol><ul><li>Entregables obligatorios:<ul><li>Notebook ejecutable (Colab o .ipynb).</li><li>Informe técnico en PDF (máx. 5 páginas).</li><li>Presentación (máx. 7 diapositivas).</li><li>Archivo ZIP opcional con modelos/artefactos.</li><li>Formulario de declaración de uso de IA (anexo).</li></ul></li><li>Envío: portal de la competencia (link en el correo de confirmación) antes de las 14:00 del 12 de noviembre. Entregas tardías no serán aceptadas salvo autorización expresa por caso documentado.</li></ul><ol start=22><li>Condiciones para premios y elegibilidad</li></ol><ul><li>Para optar a premios: todos los integrantes deben acreditar ser estudiantes activos de preparatoria en CDMX con credencial vigente.</li><li>Requisito adicional: alcanzar un mínimo de 300 puntos totales.</li><li>En caso de incumplimiento de requisitos, el premio pasará al siguiente equipo elegible.</li></ul><ol start=23><li>Premios (estructura, sin montos concretos)</li></ol><ul><li>Primer lugar: reconocimiento oficial, diploma, y premio material/beneficio.</li><li>Segundo lugar: reconocimiento y premio.</li><li>Tercer lugar: reconocimiento y premio.</li><li>Menciones especiales: por mejor análisis, mejor uso responsable de IA y mejor reproducibilidad.</li></ul><ol start=24><li>Entrega de premios</li></ol><ul><li>Fecha y lugar: se anunciará tras cierre de apelaciones.</li><li>Documentación requerida: identificación oficial o credencial escolar con foto de cada integrante.</li><li>Fiscalidad: los ganadores serán responsables de cumplir obligaciones fiscales aplicables; la organización puede requerir documentación adicional.</li></ul><ol start=25><li>Política de privacidad y manejo de datos personales</li></ol><ul><li>Los datos personales serán tratados conforme a la política de privacidad de la Universidad Iberoamericana: [enlace a política de privacidad] (insertar URL).</li><li>Los participantes autorizan el uso de su nombre, imágenes y proyectos para fines de difusión del evento.</li></ul><ol start=26><li>Responsabilidad legal, exención e indemnización</li></ol><ul><li>La organización no se hace responsable por daños personales, pérdida de equipo o datos.</li><li>Participar implica aceptar liberar de responsabilidad a la organización salvo por negligencia comprobada.</li><li>Los participantes indemnizarán a la organización por reclamaciones derivadas de violaciones a derechos de terceros.</li></ul><ol start=27><li>Accesibilidad y acomodaciones</li></ol><ul><li>Se deben solicitar adaptaciones (acceso, tiempos, instalaciones) en registro con mínimo 7 días de anticipación.</li><li>La organización hará esfuerzos razonables para garantizar accesibilidad.</li></ul><ol start=28><li>Conflictos de interés y patrocinadores</li></ol><ul><li>Los patrocinadores serán listados públicamente; miembros del jurado con conflicto de interés deberán declararlo y abstenerse de evaluar equipos afectados.</li><li>Se deberá declarar afinidad institucional si aplica.</li></ul><ol start=29><li>Soporte técnico durante la competencia</li></ol><ul><li>Contacto de soporte técnico en sitio y canal de Slack/Discord/Teams durante el evento.</li><li>Horario de soporte: 07:30–15:30 el día del evento.</li><li>En caso de falla de la plataforma oficial, la organización comunicará instrucciones alternativas.</li></ul><ol start=30><li>Información fiscal y responsabilidades sobre premios</li></ol><ul><li>El valor de premios podrá estar sujeto a impuestos según la legislación vigente; ganadores deberán proporcionar información fiscal si la organización lo solicita.</li></ul><ol start=31><li>Procedimiento de sustitución y ausencias</li></ol><ul><li>Si un integrante no puede asistir, se permite sustitución antes del inicio del evento; no se permiten cambios de integrantes durante el evento salvo por autorización médica acreditada.</li></ul><ol start=32><li>Documentos y formularios anexos (obligatorios/descargables)</li></ol><ul><li>Anexo A: Formulario de inscripción (descarga/ejemplo).</li><li>Anexo B: Declaración de uso de IA (plantilla incluida más abajo).</li><li>Anexo C: Hoja de evaluación del jurado (plantilla incluida más abajo).</li><li>Anexo D: Formato de entrega y checklist de reproducibilidad.</li></ul><ol start=33><li>Cláusula de aceptación</li></ol><ul><li>La participación en el concurso supone la aceptación de todas las presentes bases y sus anexos. El incumplimiento puede llevar a sanciones, incluida la descalificación.</li></ul><ol start=34><li>Contacto para aclaraciones</li></ol><ul><li>Joshua Haase — <a href=mailto:joshua.haase@ibero.mx>joshua.haase@ibero.mx</a> — 55 5950 4000 ext. 4056</li><li>Felipe Trujillo — <a href=mailto:felipe.trujillo@ibero.mx>felipe.trujillo@ibero.mx</a> — 55 5950 4000 ext. 7342</li><li>Soporte técnico (día del evento): <a href=mailto:soporte.tecnico@ibero.mx>soporte.tecnico@ibero.mx</a> — teléfono disponible en el correo de confirmación.</li></ul><ol start=35><li>Anexos y plantillas</li></ol><p>Anexo B — Plantilla: Declaración de uso de Inteligencia Artificial Generativa (a incluir en el informe final)</p><ul><li>Título del proyecto:</li><li>Equipo: (nombres y escuela)</li><li>Herramientas de IA utilizadas (lista con versión):<ul><li>Ejemplo: ChatGPT (OpenAI), versión/fecha: __________</li></ul></li><li>Finalidad del uso (marcar y describir):<ul><li>Redacción: [ ] breve descripción</li><li>Sugerencias de visualización: [ ] breve descripción</li><li>Apoyo conceptual: [ ] breve descripción</li><li>Otro (especificar): __________</li></ul></li><li>Fragmentos / outputs generados por IA (copiar texto o referencias):<ul><li>Output 1: ____________________</li><li>Output 2: ____________________</li></ul></li><li>Cómo se validó y adaptó el contenido generado por IA:<ul><li>Explicación técnica breve de la verificación (tests, comparación con resultados propios, cambios realizados).</li></ul></li><li>Declaración de comprensión y autoría:<br>“El equipo declara que todo el trabajo técnico clave (diseño de features, construcción y validación del modelo, interpretación de resultados) fue realizado y comprendido por los integrantes del equipo. Los contenidos generados por IA que se utilizaron fueron revisados, adaptados y referenciados tal como se describe arriba.”</li><li>Firma (integrantes del equipo, nombre y fecha):</li></ul><p>Anexo C — Plantilla: Hoja de evaluación del jurado (por equipo)</p><ul><li>Identificación del equipo:<ul><li>Nombre del equipo:</li><li>Integrantes:</li><li>Escuela:</li></ul></li><li>Evaluador: nombre y especialidad:</li></ul><p>Puntuación (valores y comentarios)</p><ol><li><p>Rendimiento del modelo (250 pts)</p><ul><li>Métrica principal (F1 phishing): ____ / 250</li><li>Comentarios: ________________________________________</li></ul></li><li><p>Calidad del análisis y EDA (100 pts)</p><ul><li>Claridad de insights: ____ / 50</li><li>Creatividad en features: ____ / 50</li><li>Comentarios: ________________________________________</li></ul></li><li><p>Reproducibilidad y código (75 pts)</p><ul><li>Notebook ejecutable: ____ / 30</li><li>Documentación y reproducibilidad: ____ / 30</li><li>Limpieza de código: ____ / 15</li><li>Comentarios: ________________________________________</li></ul></li><li><p>Reporte y presentación (50 pts)</p><ul><li>Claridad y estructura: ____ / 25</li><li>Justificación de decisiones y conclusiones: ____ / 25</li><li>Comentarios: ________________________________________</li></ul></li><li><p>Transparencia y ética/uso de IA (25 pts)</p><ul><li>Declaración de IA completa y adecuada: ____ / 15</li><li>Evidencia de comprensión y validación: ____ / 10</li><li>Comentarios: ________________________________________</li></ul></li></ol><p>Penalizaciones (indicar si aplica)</p><ul><li>Penalización por uso indebido de IA / plagio / asistencia externa: ____ puntos (justificación)</li><li>Penalización por entrega tardía: ____ puntos</li></ul><p>Puntaje total (suma) ____ / 500</p><p>Decisión del jurado: ____________________<br>Comentarios finales y recomendaciones: _______________________________</p><p>Firmas de jurados: ______________________ Fecha: __________</p><ol start=36><li>Notas de implementación y asuntos pendientes a definir (pendientes por la organización)</li></ol><ul><li>Insertar URL de la política de privacidad.</li><li>Determinar premios concretos y logística de entrega.</li><li>Confirmar miembros del jurado y comité de apelaciones.</li><li>Establecer canal de soporte técnico (Slack/Discord/Teams) y número de contacto en sitio.</li></ul><ol start=37><li>Declaración final</li></ol><ul><li>El simple hecho de participar en este concurso supone la expresa conformidad de los participantes con las presentes bases y anexos.</li></ul><h1 id=convocatoria--hackatón-detectives-de-datos><a href=#convocatoria--hackat%c3%b3n-detectives-de-datos alt>Convocatoria — Hackatón “Detectives de Datos”</a> <a href=# alt="Regresar al inicio">↑</a></h1><p>La Universidad Iberoamericana, a través del Departamento de Ingeniería para la Innovación y la Coordinación de Ciencia de Datos, convoca a estudiantes de nivel preparatoria de la Ciudad de México a participar en la hackatón Detectives de Datos (comunicaciones alternativas: Detectives de Datos: Clasifica y Protege; Ibero Data Detectives; PhishGuard: Hackatón de Clasificación de Phishing). El objetivo del evento es promover la conciencia sobre la ciencia de datos y su impacto en la seguridad digital mediante una competencia centrada en el análisis y la clasificación de correos electrónicos en dos clases: normal y phishing. Los equipos realizarán un análisis guiado, entregarán un reporte técnico y/o un modelo capaz de clasificar correos, y mostrarán reproducibilidad y justificación técnica de sus decisiones.</p><p>El evento se realizará el 12 de noviembre de 08:00 a 14:00 (CDMX) en formato de una sola ronda presencial y/o en línea. El registro previo es obligatorio y permanecerá abierto hasta el 8 de noviembre a las 23:59 en <a href=https://preuniversitarios.ibero.mx target=_blank rel=noopener>https://preuniversitarios.ibero.mx</a>. La organización publicará resultados provisionales el 14 de noviembre a las 18:00; el período de apelaciones será del 14 al 16 de noviembre (48 horas) y los resultados finales y la logística de entrega de premios se anunciarán tras el cierre de apelaciones.</p><p>Podrán inscribirse equipos de 2 a 4 integrantes; todos los miembros deben ser estudiantes activos de preparatoria con credencial vigente de la Ciudad de México. Se permite hasta una sustitución por equipo antes del inicio del evento previa notificación y autorización de la organización. Cada institución podrá inscribir varios equipos, pero solo dos equipos por institución como máximo podrán conservar plaza en la final/premiación según la regla de límite por institución; si en la lista de finalistas hay más de dos equipos de la misma escuela, solo los dos mejor ubicados conservarán la plaza y las plazas vacantes se reasignarán.</p><p>Durante la jornada (08:00–14:00) los equipos dispondrán de seis horas para realizar el trabajo y entregar los siguientes entregables obligatorios: notebook ejecutable (Colab/Jupyter .ipynb) reproducible con instrucciones claras de ejecución, informe técnico en PDF de hasta 5 páginas y presentación máxima de 7 diapositivas. Opcionalmente pueden incluir un enlace a una demo ejecutable o un ZIP con artefactos. El envío se realizará vía el portal de la competencia (link en el correo de confirmación) antes de las 14:00 del 12 de noviembre; entregas tardías no se aceptarán salvo autorización expresa con causa documentada.</p><p>Se recomienda trabajar en Google Colab o vega.ibero.mx (JupyterLab). Se permiten herramientas externas (Python, R, RStudio, Tableau, Weka, TensorFlow, PyTorch, etc.) siempre que el notebook entregado sea reproducible. El equipo debe aportar laptop con conexión a Internet; la organización ofrecerá red local y estaciones cuando sea posible. Se recomienda contar con cuenta Google para Colab. Durante la competencia habrá soporte técnico en sitio y en canal oficial (horario de soporte: 07:30–15:30).</p><p>La conducta debe ser respetuosa en todo momento. Está prohibido registrarse en múltiples equipos, compartir la misma solución entre equipos, recibir ayuda técnica pagada o no declarada, suplantar identidad, presentar trabajo ajeno sin atribución y el uso no autorizado de IA (ver política de IA). La organización podrá monitorear logs, versiones de código y solicitar ejecución en vivo de fragmentos; el uso de automatización no autorizada o explotación de sistemas será sancionado o descalificado. Cualquier código no original debe citarse en notebook e informe.</p><p>Política resumida sobre uso de Inteligencias Artificiales Generativas: se permite el uso de IA para soporte en redacción, sugerencias de visualización o apoyo conceptual, pero está prohibido que la IA genere directamente modelos, análisis clave o resultados sin revisión y comprensión por parte del equipo. Todo uso de IA debe declararse en el informe (herramienta, versión, propósito, fragmentos generados y cómo se validó/adaptó). Existirá una plantilla de “Declaración de uso de IA” en los anexos y el incumplimiento puede implicar penalización o descalificación.</p><p>Sistema de puntuación y criterios de evaluación: la puntuación máxima es 500 puntos distribuidos así: rendimiento del modelo (métrica principal F1‑score en clase “phishing”, validación en hold‑out o CV) 250 pts (50%); calidad del análisis y EDA (insights, feature engineering) 100 pts (20%); reproducibilidad y código (notebook ejecutable, documentación) 75 pts (15%); reporte y presentación 50 pts (10%); transparencia y ética/uso de IA 25 pts (5%). La evaluación incluye evaluación automática del rendimiento contra un conjunto de prueba no divulgado y revisión manual por un jurado de especialistas en ciencia de datos e ingeniería de seguridad. Penalizaciones por plagio, uso indebido de IA o asistencia externa pueden conllevar deducciones o descalificación.</p><p>Criterios de desempate: 1) mayor recall en clase “phishing”; 2) menor complejidad del modelo; 3) mejor puntaje en reproducibilidad y código; 4) fecha y hora de entrega (más temprano). Si persiste empate, el jurado decidirá con justificación pública.</p><p>Requisitos para optar a premios: todos los integrantes deben acreditar su condición de estudiantes activos de preparatoria en CDMX con credencial vigente. Es necesario alcanzar un mínimo de 300 puntos totales para ser elegible a la premiación. Premios: reconocimiento oficial, diplomas y premios materiales/beneficios para primer, segundo y tercer lugar; menciones especiales para mejor análisis, mejor uso responsable de IA y mejor reproducibilidad. La fecha y lugar de entrega de premios se anunciarán tras cierre de apelaciones; los ganadores deberán acreditar identidad y serán responsables de obligaciones fiscales aplicables.</p><p>Proceso de apelaciones: se abre un plazo de 48 horas desde la publicación de resultados provisionales; las apelaciones se envían a <a href=mailto:appeals.ibero@ibero.mx>appeals.ibero@ibero.mx</a> con asunto “Apelación Detectives de Datos – [Equipo]” e incluyen motivo y evidencia. Un comité de apelaciones independiente resolverá en un máximo de 72 horas tras el cierre del período de apelaciones; su decisión será final.</p><p>La organización publicará la política de privacidad aplicable y tratará los datos personales conforme a la misma; los participantes autorizan el uso de su nombre, imágenes y proyectos para difusión del evento. La participación implica la aceptación de estas bases y anexos; el incumplimiento podrá resultar en sanciones o descalificación. La organización no se responsabiliza por daños personales, pérdida de equipo o datos; participar supone liberar a la organización de responsabilidad salvo por negligencia comprobada y aceptar la obligación de indemnizar por violaciones a derechos de terceros.</p><p>Accesibilidad: las adaptaciones deben solicitarse en el registro con al menos 7 días de anticipación. Conflictos de interés: patrocinadores y jurado serán publicados; miembros con conflicto deberán declararlo y abstenerse de evaluar equipos afectados. Soporte técnico durante la competencia estará disponible en los canales oficiales; en caso de falla de la plataforma se comunicarán instrucciones alternativas.</p><p>Entregables y formatos obligatorios: notebook ejecutable (.ipynb o enlace Colab), informe técnico PDF (máx. 5 páginas), presentación (máx. 7 diapositivas), formulario de declaración de uso de IA (Anexo B). El portal de envío cerrará a las 14:00 del 12 de noviembre; entregas tardías no serán aceptadas salvo autorización expresa.</p><p>Anexos disponibles: Anexo A (formulario de inscripción), Anexo B (plantilla de Declaración de uso de IA), Anexo C (hoja de evaluación del jurado), Anexo D (checklist de reproducibilidad). Pendientes por definir por la organización: URL de política de privacidad, montos y logística concreta de premios, confirmación de jurado y comité de apelaciones, y canal oficial de soporte técnico.</p><p>Contactos para aclaraciones:</p><ul><li>Joshua Haase — <a href=mailto:joshua.haase@ibero.mx>joshua.haase@ibero.mx</a> — 55 5950 4000 ext. 4056</li><li>Felipe Trujillo — <a href=mailto:felipe.trujillo@ibero.mx>felipe.trujillo@ibero.mx</a> — 55 5950 4000 ext. 7342</li></ul><p>La participación en Detectives de Datos implica la aceptación expresa de las presentes bases y sus anexos.</p><p>Ayúdame a darle un título al evento donde se muestran los resultados de una competencia remota que se decide con una exposición de conclusiones y modelos de análisis de ciencia de datos para clasificar</p><p>Formal / Académico</p><ul><li>Jornada de Resultados: Clasificación y Modelos de Ciencia de Datos</li><li>Simposio de Resultados en Clasificación: Modelos y Conclusiones</li><li>Congreso de Presentación de Modelos y Hallazgos en Clasificación</li></ul><p>Técnico / Profesional</p><ul><li>DataClass Showcase: Exposición de Modelos y Resultados</li><li>ModelEval Remote: Presentación de Soluciones de Clasificación</li><li>ClasificaT: Demostración de Modelos y Conclusiones de Data Science</li></ul><p>Creativo / Atractivo</p><ul><li>ClasificaLab: Demo de Modelos y Conclusiones</li><li>HackClass: Presentación de Campeones en Clasificación</li><li>Predicciones en Escena: Muestra de Modelos y Resultados</li></ul><p>Ejecutivo / Corporativo</p><ul><li>Decision Day: Resultados y Modelos de Clasificación</li><li>Insights & Models: Jornada de Presentación de Resultados</li><li>Boardroom Data: Exposición Final de Clasificación</li></ul><p>Corto / Marca</p><ul><li>ClassConf</li><li>ModelShow</li><li>DataDecide</li><li>ClassReveal</li></ul><p>Bilingüe</p><ul><li>Classification Showcase: Resultados y Conclusiones</li><li>Data Science Finals: Presentación de Modelos de Clasificación</li></ul><p>¿Público objetivo (académico, empresas, comunidad general) y tono preferido (formal, técnico, creativo)?</p><p>Ayúdame a pensar cómo hacer emocionante el participar en un hackatón donde se analizan mensajes de spam/normales para aprender algo de ellos y clasificarlos.</p><p>Objetivo: convertir el hackatón en una experiencia competitiva, educativa y memorable alrededor del análisis y clasificación de mensajes (spam vs normal). A continuación ideas concretas, formatos, retos y criterios para maximizar emoción, aprendizaje y resultados útiles.</p><ol><li>Formato general y narrativa</li></ol><ul><li>Tema central / storytelling: “Spam Hunters”, “Detectives de Fraude” o “Bot Wars”. Cada equipo representa una agencia que debe detectar y mitigar una oleada de spam.
Añade “misiones” y resultados narrativos según desempeño.</li><li>Etapas: onboarding (datasets y tutoriales), sprint de ingeniería (feature/model), desafío sorpresa (twist en los datos), presentación final + demo en vivo.</li><li>Duración típica: 24–48 h (o compresión a 8 h con sprints). Incluir checkpoints y “drops” (datasets extra o cambios inesperados).</li></ul><ol start=2><li>Gamificación y sistema de puntos</li></ol><ul><li>Puntos por métricas técnicas (F1 macro, recall de spam, AUC), por mejoras sobre baseline (+X% sobre logistic baseline), por innovación (explicabilidad, UI), por velocidad (latencia &lt; Y ms), por uso eficiente de recursos.</li><li>Recompensas por mini-retos durante el evento: “mejor visualización”, “peor tasa de falsos positivos reducida”, “modelo más robusto a obfuscación”.</li><li>Leaderboard en vivo que muestre métricas parciales (validación) y ranking. Mantener un test oculto para evaluación final.</li></ul><ol start=3><li>Retos y actividades concretas (lista accionable)</li></ol><ul><li>Baseline Battle: proporciona baseline reproducible (TF-IDF + LR). Meta: batirlo. Puntos por cada +1% F1.</li><li>Mystery Testset (hidden labels): evalúa generalización real.</li><li>Obfuscation Challenge: mensajes intencionalmente alterados (leet speak, espacios, Unicode homoglyphs). Puntos por robustez.</li><li>Low-resource / transfer: pocas etiquetas en idioma X; usar transfer learning o Data Augmentation.</li><li>Human-in-the-loop: sistema que pide verificación humana para casos ambiguos; puntuación por eficiencia humana+modelo (reduce falsos positivos).</li><li>Adversarial Round: organizadores introducen spam adversarial y premian los modelos que lo detecten.</li><li>Explainability task: explicar por qué 5 mensajes fueron clasificados como spam; puntúa claridad y fidelidad (usar LIME/SHAP).</li><li>Deployment / Demo: puntos adicionales por app o REST API funcional con UI para visualizar decisiones.</li><li>Data quality / labeling task: recompensa por mejorar etiquetas (corrección y documentación).</li></ul><ol start=4><li>Incentivos no-técnicos</li></ol><ul><li>Premios variados: “Mejor Modelo”, “Mejor Insight de Datos”, “Mejor UX/Demo”, “Mejor Ética/Privacidad”, “Mejor Equipo Novato”.</li><li>Mentores y charlas relámpago: NLP, ML ops, seguridad, ética del dato.</li><li>Show & Tell en streaming con votación pública para premios de audiencia.</li></ul><ol start=5><li>Dataset y preparación</li></ol><ul><li>Entregar 3 particiones: train etiquetado, dev, hidden test. Incluir unlabeled pool para semi-supervised.</li><li>Añadir metadatos: timestamp, fuente, país, canal (SMS/email), longitud, presencia de links, idiomas. Facilita features no-textuales.</li><li>Proveer heurísticas y reglas (keywords) como punto de partida.</li><li>Asegurar anonimización y cumplimiento legal (no exponer números o PII real).</li><li>Inclusión de mensajes legítimos difíciles (promociones, notificaciones) para forzar trade-offs.</li></ul><ol start=6><li>Métricas y criterios de evaluación</li></ol><ul><li>Métricas principales: F1-score (priorizar recall de spam o precisión según objetivo), precision@k, AUC.</li><li>Métricas de negocio: costo de falso positivo (bloquear usuario legítimo), costo de falso negative (spam que pasa).</li><li>Compact score final: 0.6<em>F1 + 0.2</em>robustez (obfuscation) + 0.2*explainability/deploy.</li><li>Penalizaciones por overfitting al dev set (evaluación en test oculto).</li></ul><ol start=7><li>Técnicas y baselines sugeridos (rápido)</li></ol><ul><li>Baseline: TF-IDF + Logistic Regression / SVM.</li><li>Mejora: embeddings (fastText, Word2Vec) + XGBoost; transformers (fine-tune BERT/DistilBERT).</li><li>Semi-supervised: pseudo-labeling, self-training, consistency training.</li><li>No-supervised: topic modeling (LDA), clustering+analista para descubrir patrones.</li><li>Robustez: text normalization, char-level CNNs, adversarial training.</li><li>Interpretabilidad: SHAP, LIME, reglas extraídas con Decision Trees.</li></ul><ol start=8><li>Visualización y storytelling de resultados</li></ol><ul><li>Dashboard en vivo: distribución por hora, wordclouds por cluster, t-SNE/UMAP de embeddings.</li><li>“Casos de estudio” que muestren ejemplos de falsos positivos/negativos con explicación.</li><li>Línea temporal de ataques simulados y respuesta del sistema.</li></ul><ol start=9><li>Riesgos, ética y controles</li></ol><ul><li>Riesgos: filtración PII, sesgo hacia ciertos idiomas/demografías, modelos que discriminan.</li><li>Controles: dataset anon., policy para no usar datos reales identificables, cláusula de uso, revisión ética y checklist.</li><li>Premios para soluciones que minimizan sesgos y que proveen explicaciones comprensibles.</li></ul><ol start=10><li>Ejemplo de calendario (24h)</li></ol><ul><li>0:00 – 0:30 Introducción + reglas + dataset + baseline code.</li><li>0:30 – 3:30 Sprint 1: features + baseline.</li><li>3:30 – 4:00 Mini-mentor session.</li><li>4:00 – 9:00 Sprint 2: model training, features avanzadas.</li><li>9:00 – 10:00 Drop: obfuscation test (mini-challenge).</li><li>10:00 – 15:00 Finalización + deploy demo.</li><li>15:00 – 17:00 Presentaciones (5–8 min c/u) + jurado + votación.</li><li>17:00 Premiación.</li></ul><ol start=11><li>Cómo maximizar aprendizaje y colaboración</li></ol><ul><li>Plantillas reproducibles (Colab/Notebooks, Docker).</li><li>Datasets y starter kits por nivel (novato/intermedio/pro).</li><li>Canales Slack/Discord y horas de mentoría.</li><li>Incentiva la documentación: cada equipo debe entregar README con decisiones.</li></ul><p>Preguntas estratégicas (necesarias para afinar propuestas)</p><ul><li>¿Duración del hackatón y perfil de participantes (estudiantes, profesionales, mixto)?
Son estudiantes de preparatoria (nivel medio superior), por lo que preferiría</li><li>¿Dispones de presupuesto para infraestructura (GPU) y premios?
Muy poco, premios pequeños y promocionales.</li><li>¿Se permite usar datos reales con PII o solo sintéticos/anónimos?
Se usarán datos reales anonimizados</li><li>¿Objetivo principal: aprendizaje, prototipos útiles, producción o investigación?
Aprendizaje</li><li>¿Idiomas de los mensajes y volumen aproximado del dataset?
Todo en español, dataset de aprox. 1000 mensajes de cada categoría</li></ul><p>Sospecho que podríamos liberar los datos 3 semanas antes
y dar un taller de análisis de datos 5 semanas antes,
para ayudar a los participantes interesados.</p><p>Si respondes estas preguntas, diseño formato, retos y métricas óptimos ajustados a tu contexto.</p><p>Resumen ejecutivo (ajustado a estudiantes de preparatoria, presupuesto mínimo, español, ~1k mensajes por categoría)</p><ul><li>Objetivo: experiencia competitiva y formativa: aprender ML/NLP básico, ingeniería de features, evaluación y ética, con resultados demostrables.</li><li>Principales constraints a mitigar: dataset relativamente pequeño (riesgo overfitting), recursos limitados (pocos GPUs), perfil escolar (necesitan guías y plantillas).</li></ul><p>Formato y narrativa</p><ul><li>Tema: &ldquo;Detectives de Fraude&rdquo; — cada equipo es una agencia. Misiones con impacto narrativo según desempeño (ej.: &ldquo;bloqueaste la campaña maliciosa X&rdquo;, &ldquo;falso positivo: cliente enojado&rdquo;).</li><li>Etapas (24 h recomendado; 8 h versión comprimida abajo):<ul><li>Onboarding (entrega dataset, notebook starter, reglas y baseline listo en Colab).</li><li>Sprint 1: features + baseline reproducible.</li><li>Drop sorpresa (obfuscation / idiomas mixtos / unlabeled pool).</li><li>Sprint 2: modelos avanzados, explainability y deploy ligero.</li><li>Demo final + jurado + votación pública.</li></ul></li><li>Checkpoints: cada 3–4 horas mini-entregas (checkpoint de validación) para leaderboard parcial.</li></ul><p>Gamificación y sistema de puntos (ajustado a aprendizaje)</p><ul><li>Sistema por módulos (puntos acumulables; leaderboard parcial con dev set; test oculto para clasificación final):<ul><li>1 punto por cada +1% de F1 sobre baseline en dev (hasta +30 puntos).</li><li>20 puntos por mejor recall de spam en test oculto (prioriza detectar spam).</li><li>15 puntos por mejor reducción de falsos positivos (importante en mundo real).</li><li>10 puntos por robustez al obfuscation drop (ver evaluación).</li><li>10 puntos por explicar correctamente 5 casos (claridad + fidelidad).</li><li>5–10 puntos por deploy funcional (API mínima o demo web).</li><li>0–10 puntos por documentación/README y reproducibilidad.</li></ul></li><li>Mini-retos (bonos rápidos): “mejor visualización” +5, “mejor insight de datos” +5, “mejor ética/privacidad” +5.</li></ul><p>Retos y actividades accionables (lista priorizada para estudiantes)</p><ul><li>Baseline Battle (obligatorio): TF-IDF + Logistic Regression en Colab. Meta: mejorar. Puntos por cada +1% F1.</li><li>Mystery Testset (oculto): evaluación final en test secreto.</li><li>Obfuscation Challenge (drop): conjunto pequeño con leet, espacios, Unicode homoglyphs; evalúa modelos en robustness_score = accuracy_obf / accuracy_clean.</li><li>Low-resource / transfer (opcional): mostrar cómo hacer augmentation o usar embeddings preentrenados (fastText español).</li><li>Human-in-the-loop (didáctico): interfaz simple donde humano valida casos ambigüos; medida = #casos humanos necesarios vs mejora en F1.</li><li>Explainability task (obligatorio): explicar 5 predicciones; rubric: relevancia de tokens, coherencia con modelo, claridad (usar LIME/SHAP).</li><li>Deployment/demo (opcional): página simple que muestre predicción + explicación de top tokens; puntos por funcionamiento y UX básico.</li><li>Data quality task (bonus): identificación y corrección de 20 etiquetas mal anotadas en un sub-pool; puntos por precisión de correcciones.</li></ul><p>Dataset y preparación (práctico)</p><ul><li>Partición recomendada (por ejemplo para 2k mensajes = 1k spam +1k normal):<ul><li>Train etiquetado: 60% (1200).</li><li>Dev público: 20% (400).</li><li>Hidden test: 20% (400, etiquetas ocultas).</li><li>Unlabeled pool adicional (200–500) para semi-supervised / augmentation.</li></ul></li><li>Metadatos incluidos: timestamp, canal (SMS/email), longitud, has_link (sí/no), idioma (es), presencia de números. Facilitará features no textuales.</li><li>Entregar: notebooks starter en Colab con baseline reproducible + scripts para preprocesamiento y evaluación.</li><li>Anonimización: eliminar/mascarar emails, números; documentar proceso de anonimización.</li><li>Recomendar ampliar dataset con ejemplos sintéticos o públicos en español si 1k categoría resulta insuficiente.</li></ul><p>Métricas y criterio de evaluación (simple y educativo)</p><ul><li>Métricas principales:<ul><li>F1 (macro o F1_spam según objetivo). Para hackatón escolar sugiero F1_macro junto a F1_spam.</li><li>Recall_spam (negociable según prioridad).</li><li>Robustez (score en obfuscation drop).</li><li>Explainability / documentación (rubric).</li></ul></li><li>Scoring final (sencillo, educativo):<ul><li>Score_final = 0.5 * F1_dev_normalizado + 0.2 * Recall_spam_test + 0.15 * Robustez + 0.15 * Explainability/Deploy</li><li>Normalizar F1_dev a 0–1. Penalización automática si diferencia dev->test grande (>10 pts) por overfitting.</li></ul></li><li>Rubric Explainability (0–10): 4 pts fidelidad técnica (SHAP/LIME plausible), 3 pts claridad para no-técnicos, 3 pts ejemplos y justificación.</li></ul><p>Tamaño y complejidad del reto (qué simplificar para preparatoria)</p><ul><li>Priorizar explicación de modelos simples (TF-IDF + LR) y uso de embeddings precomputados (fastText) antes de transformers.</li><li>Proveer scripts para features comunes (normalización, tokenización, stemming opcional, extracción de has_link/has_number).</li><li>Para obfuscation: proporcionar reglas y ejemplos en lugar de pedir inventar detectores complejos.</li></ul><p>Infraestructura y recursos (mínimos, low-cost)</p><ul><li>Recomendado: Colab + GitHub Classroom / Google Drive; no necesitas GPUs obligatorias.</li><li>Starter kit: Colab notebook con baseline, dataset (train/dev/unlabeled), evaluación, template de README, plantilla para presentación de 5 min.</li><li>Mentor channels: Discord/Slack + sesiones “office hours” cada 3–4 h.</li><li>Para demos: Heroku gratuito / Streamlit sharing / Replit para deploys ligeros.</li></ul><p>Ejemplo de calendario (24 h — ajustado a estudiantes)</p><ul><li>0:00–0:30 Introducción, reglas, entrega starter kit.</li><li>0:30–3:00 Sprint 1: EDA + baseline reproducible.</li><li>3:00–3:30 Mini-mentor Q&amp;A.</li><li>3:30–8:30 Sprint 2: mejoras de modelo, features, explainability.</li><li>8:30–9:00 Drop obfuscation + 30 min para adaptar.</li><li>9:00–12:00 Finalización, deploy mínimo y preparar demo.</li><li>12:00–14:00 Presentaciones 6–8 equipos (5–7 min c/u).</li><li>14:00 Premiación.
Versión 8 h: compactar etapas, checkpoints cada 90 min, mantener drop breve.</li></ul><p>Aprendizaje y soporte</p><ul><li>Workshop 5 semanas antes: “Intro EDA + TF-IDF + Logistic” (práctica guiada).</li><li>Entregar datos 3 semanas antes: sí, buena idea — subir sólo train + dev; mantener test oculto.</li><li>Plantillas por nivel: Novato (notebook guiado), Intermedio (incluir embeddings), Pro (ejemplos de ensembled).</li><li>Materiales: cheat-sheet de comandos Colab, guía rápida de tokenización y ejemplos de obfuscation.</li></ul><p>Riesgos, ética y controles</p><ul><li>Riesgos clave: PII filtrada, sesgo por lenguaje/región, castigos por falsos positivos.</li><li>Controles requeridos: checklist de anonimización, cláusula de uso (no divulgar dataset), revisión previa de muestras para asegurar anonimización.</li><li>Incentivar soluciones que minimicen sesgos: premio específico + penalización en rubric si soluciones dan mal rendimiento por subgrupo (ej.: mensajes por región).</li></ul><p>Premios y logística (bajo presupuesto)</p><ul><li>Premios simbólicos: medallas imprimibles, certificados, kits promocionales, acceso a mentor 1:1 post-evento.</li><li>Premios recomendados: Mejor Modelo (técnico), Mejor Explicación, Mejor UX/Demo, Equipo Más Aprendiz (novato).</li><li>Votación pública para “audience favorite” (engagement).</li></ul><p>Recomendaciones prácticas rápidas</p><ul><li>Aumentar dataset si es posible (añadir ~500 ejemplos reales o sintéticos por clase).</li><li>Priorizar reproducibilidad (Colab notebooks) y claridad en la evaluación para evitar frustraciones.</li><li>Mantener el hidden test pequeño pero representativo; comunicar claramente que leaderboard parcial es solo indicativo.</li></ul><p>Preguntas necesarias para afinar (responde 2–3)</p><ul><li>¿Número aproximado de participantes y tamaño preferido por equipo? hasta 4 personas</li><li>¿Tendrán conexión a internet confiable y acceso a Google/Colab en el evento? sí</li><li>¿Deseas priorizar recall (detectar más spam aun con falsos positivos) o minimizar falsos positivos (no molestar a usuarios legítimos)? una métrica que equilibre ambos aspectos, con más peso al recall.</li></ul><p>Si confirmas estas tres preguntas, diseño el cronograma definitivo, scoring sheet y el paquete starter (notebooks + evaluación + guías) adaptados exactamente al número de equipos y tiempo.
Intenta enfocarte en la experiencia del juego e ideas como las pruebas y los puntajes, con una idea de cómo se implementaría técnicamente.</p><p>Confirmadas las tres respuestas: equipos hasta 4 personas; acceso a Internet/Colab sí; métrica balanceada con mayor peso a recall (detectar spam). Diseño a continuación: cronograma definitivo (24 h + versión 8 h), scoring sheet claro y paquete starter técnico listo para implementar con recursos mínimos.</p><p>Cronograma definitivo (24 h) — entregables y checkpoints</p><ul><li>0:00–0:20 Kickoff (reglas, narrativa “Detectives de Fraude”, entrega starter kit con train/dev/unlabeled y baseline Colab).<ul><li>Entregable 0: repositorio del equipo + notebook baseline copiado.</li></ul></li><li>0:20–3:00 Sprint 1: EDA + baseline reproducible (TF-IDF + LR).<ul><li>Checkpoint 1 (3:00): subir a carpeta compartida un zip con notebook y predicciones dev.csv; leaderboard dev se actualiza.</li></ul></li><li>3:00–3:30 Mentor drop-in Q&amp;A.</li><li>3:30–8:30 Sprint 2: features avanzadas, embeddings (fastText precomputed), pipelines de preprocesamiento.<ul><li>Checkpoint 2 (8:30): breve README con hipótesis y pipeline.</li></ul></li><li>8:30–9:00 Drop sorpresa (Obfuscation set). 30 min para adaptar y enviar predicciones obf.csv.</li><li>9:00–12:00 Finalización: explainability (5 casos), packaging mínimo para demo (Streamlit/Replit) y README final.<ul><li>Checkpoint 3 (12:00): entregar explainability.pdf con 5 explicaciones y link demo.</li></ul></li><li>12:00–14:00 Presentaciones (5–7 min por equipo: demo + 3 diapositivas clave).</li><li>14:00–14:30 Evaluación final oculta + deliberación jurado.</li><li>14:30 Premiación y anuncios de feedback/mentorías post-hack.</li></ul><p>Versión comprimida (8 h) — checkpoints cada 90 min</p><ul><li>0:00–0:15 Kickoff + baseline.</li><li>0:15–2:00 Sprint 1: EDA + baseline.</li><li>2:00–2:20 Mentor Q&amp;A.</li><li>2:20–4:00 Sprint 2: mejoras + explainability.</li><li>4:00–4:30 Drop obfuscation (30 min adaptación).</li><li>4:30–6:30 Finalización + deploy mínimo.</li><li>6:30–8:00 Presentaciones + jurado.</li></ul><p>Scoring sheet (transparente, educativo, prioriza recall)
Formato de puntos totales: 200 puntos (fácil de repartir y explicar). Todos los sub-scores se normalizan a 0–100 y luego escala a puntos.</p><ol><li>Rendimiento en dev (leaderboard público) — 80 pts</li></ol><ul><li>F1_macro_dev normalizado (0–100) → 0.8 * 80 = hasta 64 pts</li><li>Bonus por mejora respecto baseline (TF-IDF+LR): +1 pt por cada +1% F1 sobre baseline hasta +16 pts.</li></ul><ol start=2><li>Rendimiento en test oculto (evaluación final) — 60 pts</li></ol><ul><li>Recall_spam_test (mayor peso) → 40 pts (Recall_spam_norm * 40)</li><li>Precision_spam_test → 20 pts (Precision_norm * 20)</li><li>Penalización por drop dev->test: si F1_dev - F1_test > 10 puntos → restar 10 pts por cada 5 puntos de exceso (anti-overfitting).</li></ul><ol start=3><li>Robustez al obfuscation (drop) — 30 pts</li></ol><ul><li>Robustness = accuracy_obf / accuracy_clean (o F1_obf / F1_clean). Normalizar y asignar hasta 30 pts.</li><li>Bonus +5 pts si cumple ≥90% de la performance clean.</li></ul><ol start=4><li>Explainability / Deploy / Documentación — 30 pts</li></ol><ul><li>Explainability (5 casos) — rubric 0–10:<ul><li>4 pts fidelidad técnica (uso de SHAP/LIME y coherencia con modelo)</li><li>3 pts claridad para no-técnicos</li><li>3 pts ejemplos y justificación</li></ul></li><li>Deploy/demo funcional — 10 pts (REST API o web demo con explicación visible)</li><li>README + reproducibilidad — 10 pts (instrucciones, requirements, seed, notebook runnable)</li></ul><p>Mini-bonos (hasta +10 pts)</p><ul><li>Mejor visualización de datos +5</li><li>Mejor ética / privacidad y documentación de anonimización +5</li></ul><p>Ejemplo de cálculo final</p><ul><li>Puntos_totales = Rendimiento_dev(0–80) + Rendimiento_test(0–60) + Robustez(0–30) + Exp/Deploy(0–30) + Bonos(0–10)</li><li>Orden final por Puntos_totales; leaderboard parcial muestra solo Rendimiento_dev y Robustez parcial (obf submissions).</li></ul><p>Rubric de explainability (detallado)</p><ul><li>Fidelidad técnica (0–4): presencia de saliencia/SHAP/LIME, consistencia con características más importantes.</li><li>Claridad (0–3): explicación comprensible por un profesor no-ML.</li><li>Ejemplos (0–3): 5 mensajes con explicación: tokens señalados, regla intuible, si sería acción automatizada o revisión humana.</li></ul><p>Implementación técnica del leaderboard y evaluación (low-cost)</p><ul><li>Dev leaderboard en vivo (público): permitir submissions automáticas a un endpoint mínimo:<ul><li>Opción 1 (mínima infraestructura): cada equipo sube dev_preds.csv a Google Drive carpeta con un script organizador que corre evaluación y actualiza Google Sheets (Apps Script / Colab para correr cron job cada 15–30 min).</li><li>Opción 2 (algo técnico pero gratis para small load): desplegar Flask Cloud Run / Railway/Render con endpoint upload → eval en memoria y actualizar Sheets/Google Firebase.</li></ul></li><li>Test oculto: no publicar labels; equipos suben test_preds.csv a carpeta privada; los organizadores ejecutan evaluación final y aplican penalizaciones.</li><li>Para transparencia: publicar el notebook de evaluación (sin labels test) y el script que transforma métricas a puntos.</li></ul><p>Starter package (archivo y contenido a entregar en GitHub Classroom / carpeta compartida)</p><ul><li>data/<ul><li>train.csv (labels), dev.csv (labels), unlabeled_pool.csv, obfuscation_drop.csv (sin labels para drop)</li><li>README_dataset.md (partición, anonimización, metadatos)</li></ul></li><li>notebooks/<ul><li>00_baseline_TFIDF_LR.ipynb (Colab-ready; tokenización, train, dev eval, export dev_preds.csv)</li><li>01_fasttext_embeddings_example.ipynb (cómo usar fastText precomp, simple XGBoost)</li><li>02_explainability_demo.ipynb (SHAP/LIME para 5 casos)</li><li>03_deploy_streamlit_template.ipynb (export model + demo minimal)</li></ul></li><li>eval/<ul><li>evaluate.py (script que toma preds.csv y produce métricas; documentación de inputs/outputs)</li><li>scoring_sheet.md (fórmulas de puntos)</li></ul></li><li>templates/<ul><li>presentation_template.pptx (3 diapositivas + demo)</li><li>README_template.md (qué entregar, requerimientos)</li><li>submission_template.csv (formatos de columnas: id,label)</li></ul></li><li>infra/<ul><li>leaderboard_instructions.md (cómo subir y cómo funciona)</li><li>organizer_eval_colab.ipynb (para correr evaluación fake/pública y final)</li></ul></li><li>extras/<ul><li>obfuscation_examples.txt (leet, homoglyphs, spaces, repeated punctuation)</li><li>augmentation_snippets.py (pseudo-labeling example, back-translation pointers)</li></ul></li></ul><p>Cómo evaluar robustez (implementación simple)</p><ul><li>Para cada equipo:<ul><li>Calcular F1_clean = F1 sobre dev/test limpio.</li><li>Calcular F1_obf = F1 sobre obfuscation set.</li><li>Robustness_score = (F1_obf / F1_clean) clipped at 1.0.</li><li>Normalizar a 0–100 para asignar puntos a sección de Robustez.</li></ul></li></ul><p>Consejos prácticos para estudiantes (incluir en starter kit)</p><ul><li>Scripts de preprocesado ya listos: normalización (lowercase, remove homoglyphs map), tokenización básica, detect has_link/has_number features.</li><li>fastText embeddings ya descargados y una celda para cargar y promediar vectores.</li><li>Example of pseudo-labeling: tomar unlabeled_pool, predecir con ensemble, añadir ejemplos con confianza >0.95.</li><li>Plantilla de explainability con 5 mensajes: mostrar top-5 tokens y por qué modelo decidió spam.</li></ul><p>Controles éticos y anonimización (a incluir en pack)</p><ul><li>Documento short: “Checklist anonimización” (qué se removió: números, correos, nombres).</li><li>Cláusula de uso de datos que firman equipos (no compartir públicamente).</li><li>Premio por mejor solución que minimice sesgo y documente impacto por subgrupo (ej.: rendimiento por canal o longitud).</li></ul><p>Riesgos a comunicar al inicio (breve)</p><ul><li>Dataset pequeño → riesgo de overfitting; insistir en dev/test oculto.</li><li>Obfuscation puede favorecer heurísticas simples pero enseñar robustez.</li><li>Revisar que no haya PII residual antes de publicar dataset.</li></ul><p>Acción final que yo puedo entregar si confirmas:</p><ul><li>Cronograma final en formato imprimible + versión 8 h</li><li>Scoring sheet en Google Sheets con fórmulas automáticas</li><li>Repo starter (estructura arriba) con notebooks Colab listos y evaluate.py
Indica si quieres que genere el repositorio starter ahora (subiré los notebooks y evaluate.py) y el template de Google Sheets para leaderboard.</li></ul></article></main></body></html>