<!doctype html><html lang=es-mx><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><title>Computación de Alto Rendimiento (O alta utilización)</title>
<link rel=stylesheet media=screen href=https://joshua.haase.mx/css/theme.min.2ee1317322f9eb9b2ef0a618d19b20e38c11f5f9c310751400a45db225dd2626.css integrity="sha256-LuExcyL565su8KYY0Zsg44wR9fnDEHUUAKRdsiXdJiY="></head><body><main><article><h1>Computación de Alto Rendimiento (O alta utilización)</h1><h1 id=el-problema>El problema</h1><p>Muchas aplicaciones científicas (y algunas comerciales) requieren una gran capacidad de cómputo.</p><p>Una estrategia para atacar este problema es maximizar el uso de los recursos computacionales disponibles. Esto se conoce como Cómputación de Alto Rendimiento (HPC en inglés).</p><p><a href=https://cloudscaling.com/blog/cloud-computing/grid-cloud-hpc-whats-the-diff/>El rendimiento y la escalabilidad son ortogonales</a>.</p><h1 id=retos>Retos</h1><p>Hay varios problemas que deben resolverse para generar un cluster de alto rendimiento:</p><ul><li><p>Cómo administrar todos los nodos para:</p><ul><li><p>Mantener consistencia en la configuración.</p></li><li><p>Mantener consistencia el software y aceptar distintas versiones.</p><p>Diferentes usuarios pueden necesitar software diferente y/o de distintas versiones. <a href=#fn1 class=footnote-ref id=fnref1 role=doc-noteref><sup>1</sup></a> En el cluster de NYU utilizan módulos con las diferentes versiones de software.</p></li><li><p>Mantener consistencia en usuarios y permisos.</p></li></ul></li><li><p>Cómo compartir los recursos.</p></li><li><p>Cómo asignar las tareas de manera justa.</p></li></ul><h1 id=librerías-para-cómputo-distribuído>Librerías para cómputo distribuído</h1><h2 id=openmpi><a href=https://www.open-mpi.org/ title="A High Performance Message Passing Library">OpenMPI</a></h2><p>Es una librería para generar programas que se pueden ejecutar en paralelo. Los programas deben usar la librería OpenMPI para utilizar sus ventajas.</p><h2 id=harness><a href=http://www.csm.ornl.gov/harness/>Harness</a></h2><p>Antes de que se estandarizara OpenMPI, había un proyecto llamado <a href=http://www.netlib.org/pvm3/book/node17.html>Parallel Virtual Machine</a> para utilizar varias computadoras como una sola más grande.</p><p>Fue un proyecto para la investigacion de computo heterogéneo.</p><h2 id=pmv>PMV</h2><p>Es un <a href=http://www.csm.ornl.gov/pvm/ title="Parallel Virtual Machine.">sistema para cómputo heterogéneo y concurrente</a>.</p><p><a href=http://www.csm.ornl.gov/pvm/PVMvsMPI.ps%20%22PMV%20permite%20configuraciones%20heterogéneas.%20MPI%20es%20más%20rápido%20para%20la%20mayoría%20de%20las%20aplicaciones.%22>Comparación entre PMV y MPI</a>.</p><p>Fue <a href=https://en.wikipedia.org/wiki/Parallel_Virtual_Machine>reemplazado por MPI en la mayoría de los casos</a>.</p><h2 id=general-purpose-computing-on-graphics-processing-units>General Purpose Computing on Graphics Processing Units</h2><ul><li><p><a href=http://www.khronos.org/opencl/>OpenCL</a></p><p>La especificación de OpenCL describe un lenguaje de programación, un entorno general que debe estar presente, y una API en C que permite a los programadores llamar este entorno. <a href=#fn2 class=footnote-ref id=fnref2 role=doc-noteref><sup>2</sup></a></p></li><li><p><a href=http://www.nvidia.com/object/cuda_home_new.html>CUDA</a></p><p><a href=http://www.nvidia.com/object/cuda_home_new.html title="De acuerdo al sitio web de CUDA.">Puede ejecutar programas en C, C++ y Fortran directamente</a>.</p></li></ul><h1 id=librerías-para-memoria-distribuida>Librerías para Memoria Distribuida</h1><h2 id=numa>NUMA</h2><p>Significa «Acceso de Memoria No Uniforme» (Non-Uniform Memory Access).</p><p>Puede implementarse por Hardware usando varios bus para que los procesadores accedan a la memoria (en contraposición a SMP que usa un sólo bus para todos los procesadores).</p><p>Puede usarse para dar la ilusión de que los nodos tienen más memoria de la que tienen (distributed shared memory).</p><h1 id=planificadores-de-trabajo>Planificadores de trabajo</h1><h2 id=openlava>OpenLava</h2><p>Un <a href=http://www.openlava.org/%20%22Vale%20la%20pena%20revisarlo,%20es%20libre%20y%20LSF%20parece%20fácil%20de%20usar.%22>planificador de trabajos compatible con LSF</a>, aquí <a href=http://www.openlava.org/documentation/quickstart.html%20%22%22>la guía rápida</a>.</p><p>Se puede <a href=http://www.openlava.org/tarball/openlava-2.2.tar.gz%20%22%22>descargar el código fuente aquí</a>.</p><p><a href="https://soylentnews.org/submit.pl?op=viewsub&amp;subid=16724%20%22%22">IBM demandó openlava para impedir la distribución de las versiones nuevas</a>.</p><p>https://swisstech.ca/openlava-platform-lsf-alternative/</p><h2 id=htcondor><a href=https://research.cs.wisc.edu/htcondor/index.html>HTCondor</a></h2><p>Permite la utilización de computadoras personales para CAR, y desocupa las computadoras cuando el dueño la utiliza.</p><p>Permite salvar procesos a medias y moverlos entre compus, pero se necesitan vincular con la librería de HTCondor.</p><p>Permite especificar los requisitos mínimos para cada proceso.</p><p>Para enviar trabajos se tiene que llenar una especificación un tanto compleja.</p><h2 id=torque>Torque</h2><p>Es el fork libre de OpenPBS. Tiene una versión comercial.</p><h2 id=moab>MOAB</h2><h2 id=slurm>SLURM</h2><h1 id=notas>Notas</h1><p>Al parecer, <a href=https://en.wikipedia.org/wiki/Distributed_shared_memory>compartir memoria RAM</a> es una mala idea<a href=#fn3 class=footnote-ref id=fnref3 role=doc-noteref><sup>3</sup></a> porque la transferencia en red es órdenes de magnitudes más lento que el procesamiento de RAM y el tráfico que genera entorpece la red.</p><p>Se puede generar un RAMDISK a partir de muchos nodos, pero su desempeño puede ser muy malo.</p><p>Las alternativas a compartir memoria son:</p><ul><li>Pasar mensajes (equivalente a compartir memoria),</li><li>Map - Reduce (para hacer el problema más pequeño).</li></ul><hr><p><a href=http://www.csm.ornl.gov/pvm/PVMvsMPI.ps title="Sólo una curiosidad.">WoDi utiliza Condor sobre PMV</a>.</p><h1 id=distribuciones-listas-para-cómputo-de-alto-rendimiento>Distribuciones listas para cómputo de alto rendimiento</h1><p><a href=http://www.rocksclusters.org/rocks-documentation/4.1/getting-started.html>ROCKS</a></p><h1 id=paquetes-que-ayudan-con-la-gestión>Paquetes que ayudan con la gestión</h1><p><a href=http://svn.oscar.openclustergroup.org/trac/oscar%20%22OSCAR%20allows%20users,%20regardless%20of%20their%20experience%20level%20with%20a%20*nix%20environment,%20to%20install%20a%20Beowulf%20type%20high%20performance%20computing%20cluster.%22>OSCAR</a></p><h1 id=manuales-para-configurar-un-grupo-computacional-cluster>Manuales para configurar un grupo computacional (cluster)</h1><p>http://www.debianadmin.com/how-to-set-up-a-high-performance-cluster-hpc-using-debian-lenny-and-kerrighed-updated.html</p><p>https://www-users.cs.york.ac.uk/~mjf/pi_cluster/src/Building_a_simple_Beowulf_cluster.html</p><p>https://learn.scientificprogramming.io/introduction-to-high-performance-computing-hpc-clusters-9189e9daba5a</p><p>https://wiki.debian.org/HighPerformanceComputing</p><p>http://linuxhpc.org</p><p>https://openhpc.community/</p><p>https://www.ibm.com/developerworks/library/l-cluster1/index.html</p><p>https://web.archive.org/web/20110803053915/http://www.physics.ubc.ca/mbelab/berserk_doc/steam-admin/html/administration.html#user-wall-time</p><h1 id=herramientas-para-aprovisionar>Herramientas para aprovisionar</h1><p><a href=https://www.redhat.com/en/technologies/management/satellite>Satellite</a> <a href=http://cobbler.github.io/>Cobbler</a> <a href=https://spacewalkproject.github.io/>Spacewalk</a>: la versión libre de Satellite, que incluye un inventario que podría #3 <a href=https://fai-project.org/fai-guide/>FAI</a> y <a href=https://github.com/J0oo/FAI-Administration-Tool>una posible interfaz gráfica</a> <a href=https://github.com/purpleidea/mgmt>mgmt</a> <a href=http://kadeploy3.gforge.inria.fr/>Kadeploy</a> <a href=https://github.com/yasuhito/Lucie>Lucie</a></p><h2 id=foreman><a href=https://theforeman.org/>Foreman</a></h2><p><a href=https://github.com/adfinis-sygroup/foreman-ansible>Un script de ansible para instalar the foreman</a> <a href=https://github.com/adfinis-sygroup/foreman-yml>y la configuración correspondiente</a>. <a href=https://github.com/theforeman/foreman_ansible>Un sistema para integrar ansible</a>.</p><h1 id=referencias>Referencias</h1><p>[1]: ¿Qué es OSCAR (Open Source Cluster Application Resources)? 2013-12-14 http://svn.oscar.openclustergroup.org/trac/oscar</p><pre><code>http://www.admin-magazine.com/HPC/Articles/real_world_hpc_setting_up_an_hpc_cluster</code></pre><pre><code>2016 NVIDIA Corporation
http://www.nvidia.com/object/cuda_home_new.html</code></pre><pre><code>https://en.wikipedia.org/wiki/Distributed_shared_memory</code></pre><pre><code> https://technet.microsoft.com/en-us/library/ms178144(v=sql.105 ).aspx</code></pre><h1 id=pendientes>Pendientes</h1><p><a href=http://github-pages.ucl.ac.uk/RCPSTrainingMaterials/HPCandHTCusingLegion/1_intro_to_hpc-reveal.html#/applications>Varios conceptos de HTC y HPC</a></p><p><a href=https://en.wikipedia.org/wiki/List_of_distributed_computing_projects>Lista de proyectos de cómputo distribuído</a></p><p><a href=https://aws.amazon.com/hpc/>Amazon ofrece servicio de HPC</a></p><h2 id=monitoreo>Monitoreo</h2><p>http://www.toptenreviews.com/software/privacy/best-monitoring-software/</p><p>https://en.wikipedia.org/wiki/Comparison_of_network_monitoring_systems</p><p>http://ganglia.sourceforge.net/</p><section class=footnotes role=doc-endnotes><hr><ol><li id=fn1 role=doc-endnote><p><a href=https://wikis.nyu.edu/display/NYUHPC/Finding+and+using+software+with+Environment+Modules>5</a><a href=#fnref1 class=footnote-back role=doc-backlink>↩︎</a></p></li><li id=fn2 role=doc-endnote><p><a href=https://wiki.archlinux.org/index.php/GPGPU>4</a><a href=#fnref2 class=footnote-back role=doc-backlink>↩︎</a></p></li><li id=fn3 role=doc-endnote><p><a href=http://superuser.com/questions/256521/how-to-share-cpu-or-ram>6</a>,<a href=Distributed%20Shared%20memory>7</a><a href=#fnref3 class=footnote-back role=doc-backlink>↩︎</a></p></li></ol></section></article></main></body></html>